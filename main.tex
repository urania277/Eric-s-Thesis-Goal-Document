\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{float}
\usepackage{gensymb}
\usepackage[english]{babel}
\usepackage{tabularx}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{dirtytalk}
\setlength{\columnsep}{.8cm}
\geometry{a4paper,
 total={210mm,297mm},
 left=33mm,
 right=33mm,
 top=30mm,
 bottom=30mm,}
\newcommand{\code}[1]{{\texttt{#1}}}
\usepackage{titlesec}
\title{\huge Thesis Project Goals}

\author{Eric Wulff}
\date{\today}

\begin{document}

\maketitle

\textbf{Working title:} Autoencoders for Data Compression and Anomaly Detection in High Energy Physics

\textbf{Student:} Eric Wulff, Lund University

\textbf{Principal supervisor:} Caterina Doglioni, Lund University, ATLAS Collaboration

\textbf{Preliminary start and end dates:} 01/09/2019 - 31/01/2020

\section{Background and motivation}

\subsection{CERN}

The European Organisation for Nuclear Research (CERN) is one of the largest research laboratories in the world and has as its mission to conduct world class scientific research in fundamental physics as well as to push the frontiers of science and technology. The main scientific tools used at CERN are particle accelerators and detectors. In fact, CERN is home to the largest particle accelerator in the world, the Large Hadron Collider (LHC), perhaps best known for having been used to discover the Higgs boson~\cite{higgs_discovery}.

The high technological demands of building particle accelerators for use in particle physics research result in CERN pushing the frontiers of technology, often to the benefit of society at large. As an example, the perhaps most important invention in human history, the world wide web, was first imagined at CERN \cite{www}. Medical diagnosis and treatment are other areas where CERN scientists have helped make big technological advances. Medical applications that rely on technology first developed for particle physics research includes PET scans, MRI scans (for diagnosis) and hadron therapy (cancer treatment).

\subsection{The ATLAS experiment}

ATLAS~\cite{Collaboration_2008} stands for A Toroidal LHC ApparatuS and is the largest experiment at CERN. It is a general purpose particle detector designed to detect as much as possible of the particles that gets created in the proton-proton collisions produced by the LHC.

There are approximately 1.7 billion collisions occurring inside the ATLAS detector each second~\cite{trigger_das}. Most of these collisions, however, are not interesting and therefore will not be sent to storage. In addition, current technological limitations make it impossible to store the enormous amount of data that gets produced. Hence, ATLAS uses a so called \emph{trigger system}, consisting of two levels, one hardware level and one software level. The hardware trigger comes first and saves at most $10^5$ collisions per second. Thereafter the software trigger performs additional analysis and picks out about $10^3$ events per second which are sent to a data storage system for later analysis~\cite{trigger_das}.

Even though most of the collision events gets thrown away by the trigger system the remaining data that is saved quickly adds up to enormous amounts. Therefore it is of high interest to compress this data as much as possible without loosing valuable information.

If we manage to store events in a smaller space we can afford to include more information in each event, for instance not only the jets as a whole but perhaps also their constituents could be saved. This would allow for physics analysis not previously possible such as to look for new particles, e.g. dark matter, in the substructure of jets.

\section{Objectives and research questions}

The objective is to compress experimental data from the ATLAS detector without loosing important information. It is desirable to save as much final-state information as possible while filtering out noise.

\section{Approach and method}

A machine learning (ML) approach will be used to complete the objective, specifically a method using so called autoencoder (AE) neural networks. In short an AE is a neural network which tries to implement an approximation to the identity, $f(x) \approx x$, by using one or more hidden layers with smaller size than the input and output layers. If this is possible, it means that all the information necessary to reproduce the input, $x$, is contained in the hidden layer. Consequently the information has been compressed. The idea is then to only save this hidden layer representation along with the neural network that can recreate the original data.

The reasoning above relied on the hidden layer being smaller than the input and output layers in order to achieve compression. This is however not necessary if one instead impose some other restraint on the network, for instance that the average activation of neurons in the network should be lower than a given number~\cite{ng_autoenc}.

\section{Disciplinary foundation}
It has been shown that AEs can be used to reduce the dimensionality of data \cite{hinton}. 

\section{Contribution to the development of knowledge}

It will be the first time AEs are used to compress data within the ATLAS collaboration. The project will shed light on how useful AEs can be for compression of experimental high energy physics data.

\section{Required resources}

\begin{itemize}
    \item NVIDIA GPU for training neural networks
    \item Access to experimental data from ATLAS
\end{itemize}



% \begin{figure}[h]
% \centering
% \includegraphics[width=0.9\textwidth,trim={0mm 0mm 0mm 0mm},clip]{rect_dipole_450GeV_HL_vs_SEY.png}
% \caption{The heat load on the beam screen due to electron bombardment as a function of the SEY parameter.}
% \label{fig1}
% \end{figure}

\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
